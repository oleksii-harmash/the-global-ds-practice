<h1 align="center">Дерево принятия решений</h1>  
Дерево решений — метод представления решающих правил в определенной иерархии, включающей в себя элементы двух типов — узлов (node) и листьев (leaf). Узлы включают в себя решающие правила и производят проверку примеров на соответствие выбранного атрибута обучающего множества.  
  
 
    Деревья принятия решений являются универсальными алгоритмами машинного обучения, которые могут заниматься задачами классификации и регрессии. Они представляют собой очень мощные алгоритмы, способные подгоняться к сложным наборам данных. Деревья решений также являются фундаметальными компонентами случайных лесов, которые входят в число самых мощных алгоритмов машинного обучения, доступных на сегодняшний день. 

Дерево решений само по себе — это в основном жадное, нисходящее, рекурсивное разбиение. «Жадное», потому что на каждом шагу выбирается лучшее разбиение. «Сверху вниз» — потому что мы начинаем с корневого узла, который содержит все записи, а затем делается разбиение.

Простой случай: примеры попадают в узел, проходят проверку и разбиваются на два подмножества:
* первое — те, которые удовлетворяют установленное правило;
* второе — те, которые не удовлетворяют установленное правило.  
  

![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/rfc_vs_dt11.png)   

Далее к каждому подмножеству снова применяется правило, процедура повторяется. Это продолжается, пока не будет достигнуто условие остановки алгоритма. Последний узел, когда не осуществляется проверка и разбиение, становится листом.

Лист определяет решение для каждого попавшего в него примера. Для дерева классификации — это класс, ассоциируемый с узлом, а для дерева регрессии — соответствующий листу модальный интервал целевой переменной. В листе содержится не правило, а подмножество объектов, удовлетворяющих всем правилам ветви, которая заканчивается этим листом.

Пример попадает в лист, если соответствует всем правилам на пути к нему. К каждому листу есть только один путь. Таким образом, пример может попасть только в один лист, что обеспечивает единственность решения.

### Терминология
Изучите основные понятия, которые используются в теории деревьев решений, чтобы в дальнейшем было проще усваивать новый материал.

![](https://habrastorage.org/getpro/habr/upload_files/5b7/05c/282/5b705c28291baee36340d6e42d4b045a)

### Какие задачи решает дерево решений?
Его применяют для поддержки процессов принятия управленческих решений, используемых в статистистике, анализе данных и машинном обучении. Инструмент помогает решать следующие задачи:

* **Классификация**. Отнесение объектов к одному из заранее известных классов. Целевая переменная должна иметь дискретные задачи.

* **Регрессия** (численное предсказание). Предсказание числового значения независимой переменной для заданного входного вектора.

* **Описание объектов**. Набор правил в дереве решений позволяет компактно описывать объекты. Поэтому вместо сложных структур, используемых для описания объектов, можно хранить деревья решений.

### Важные теоретические определения

**Энтропия**

**Энтропия** — это мера случайности или неопределенности. Уровень энтропии колеблется от 0 до 1. Когда энтропия равна 0, это означает, что подмножество чистое, то есть в нем нет случайных элементов. Когда энтропия равна 1, это означает высокую степень случайности. Энтропия обозначается символами *H(S)*.

*Говоря простым языком, энтропия – это не что иное, как мера беспорядка. (Еще ее можно считать мерой чистоты)*

**Формула энтропии Шеннона**

![](https://habrastorage.org/r/w1560/webt/ey/wa/u-/eywau-ntm5stedcuyrelbhhoipu.png)

Здесь $p_i$ – это частотная вероятность элемента/класса и наших данных. Для простоты, предположим, что у нас всего два класса: положительный и отрицательный. Тогда i будет принимать значение либо «+», либо «-». Если бы у нас было в общей сложности 100 точек в нашем наборе данных, 30 из которых принадлежали бы положительному классу, а 70 – отрицательному, тогда p+ было бы равно 3/10, а p- будет 7/10. Тут все просто.

Если вычислять энтропию классов из этого примера, то вот, что получится, воспользовавшись формулой выше:

![](https://habrastorage.org/r/w1560/webt/5_/hh/20/5_hh20bihmp119n_5vzmlq_vuyw.png)

Энтропия составит примерно 0,88. Такое значение считается довольно высоким, то есть у нас высокий уровень энтропии или беспорядка (то есть низкое значение чистоты). Энтропия измеряется в диапазоне от 0 до 1. В зависимости от количества классов в вашем наборе данных значение энтропии может оказаться больше 1, но означать это будет все то же, что уровень беспорядка крайне высок.

![](https://habrastorage.org/r/w1560/webt/jt/sx/zs/jtsxzsfwwbstp10fqo-rd0ndddw.png)

На оси X отражено количество точек из положительного класса в каждой окружности, а на оси Y – соответствующие им энтропии. Вы сразу же можете заметить перевернутую U-образную форму графика. Энтропия будет наименьшей в экстремумах, когда внутри окружности нет положительных элементов в принципе, либо когда в них только положительные элементы. То есть, когда в окружности одинаковые элементы – беспорядок будет равен 0. Энтропия окажется наиболее высокой в середине графика, где внутри окружности будут равномерно распределены положительные и отрицательные элементы. Здесь будет достигаться самая большая энтропия или беспорядок, поскольку не будет преобладающих элементов.

### Информационный выигрыш

Дальше нам понадобится величина для измерения уменьшения этого беспорядка в дополнительной информации (признаках/независимых переменных) целевой переменной/класса. Вот тут в игру вступает Information Gain или информационный выигрыш. С точки зрения математики его можно записать так:

![](https://habrastorage.org/webt/bn/el/t4/bnelt40yxay8hkbanig088mpk6a.png)

### Как работает дерево решений

Набор данных:

![](https://habrastorage.org/r/w1560/webt/o0/iw/hx/o0iwhxmogg6mmul7q5ahzgsrx_e.png)

В нашем наборе данных два атрибута, BMI и Age. В базе данных семь записей. Построим дерево решений для нашего набора данных.

**1. Корневой узел**

В дереве решений начнем с корневого узла. Возьмем все записи (в нашем наборе данных их семь) в качестве обучающих выборок.

    В корневом узле наблюдаем три голоса за и четыре против.
    Вероятность принадлежности к классу 0 равна 4/7. Четыре из семи записей принадлежат к классу 0.
    P(0) = 4/7
    Вероятность принадлежности к классу 1 равна 3/7. То есть три из семи записей принадлежат классу 1.
    P(1) = 3/7.
    
Вычисляем энтропию корневого узла:

![](https://habrastorage.org/r/w1560/webt/pj/dz/bo/pjdzboaros5outiyyut4l-o-abg.png)

**2. Как происходит разбиение?**

У нас есть два атрибута — BMI и Age. Как на основе этих атрибутов происходит разбиение? Как проверить эффективность разбиения?

1. При выборе атрибута BMI в качестве переменной разделения и ≤30 в качестве точки разделения мы получим одно чистое подмножество.

Точки разбиения рассматриваются для каждой точки набора данных. Таким образом, если точки данных уникальны, то для n точек данных будет n-1 точек разбиения. То есть в зависимости от выбранных точки и переменной разбиения мы получаем высокий информационный выигрыш и выбираем разделение с этим выигрышем. В большом наборе данных принято считать только точки разделения при определенных процентах распределения значений: 10, 20, 30%. У нас набор данных небольшой, поэтому, видя все точки разделения данных, выбираем в качестве точки разделения значения ≤30.

![](https://habrastorage.org/r/w1560/webt/wr/xs/wl/wrxswlfaxle2k5v1mpsu5pk39ne.png)

2. Энтропия чистого подмножества равна нулю. Теперь рассчитаем энтропию другого подмножества. Здесь у нас три голоса за и один против.

        P(0)=1/4 (одна из четырех записей)
        P(1)=3/4 (три из четырех записей)
        
![](https://habrastorage.org/r/w1560/webt/zq/e8/xo/zqe8xolanqfuytbwu8zomgqhegw.png)

Чтобы решить, какой атрибут выбрать для разбиения, нужно вычислить информационный выигрыш.

![](https://habrastorage.org/r/w1560/webt/yb/tg/le/ybtgleedo45dolcfaie1or8_c1i.png)

2. Выберем атрибут Age в качестве переменной разбиения и ≤45 в качестве точки разбиения.

![](https://habrastorage.org/r/w1560/webt/qj/kg/hl/qjkghlrohuzyjfvf4lvdu16oteo.png)

Давайте сначала вычислим энтропию подмножества True. У него есть одно да и одно нет. Это высокий уровень неопределенности. Энтропия равна 1. Теперь рассчитаем энтропию подмножества False. В нем два голоса за и три против.

![](https://habrastorage.org/r/w1560/webt/gs/or/li/gsorliraqtuyvqgs4tz6yboqozq.png)

3. Рассчитаем информационный выигрыш.

![](https://habrastorage.org/r/w1560/webt/re/o2/it/reo2itwhb9vdkun9s_1wzt0mafk.png)

Мы должны выбрать атрибут, имеющий высокий информационный выигрыш. В нашем примере такую ценность имеет только атрибут BMI. Таким образом, атрибут BMI выбирается в качестве переменной разбиения. После разбиения по атрибуту BMI мы получаем одно чистое подмножество (листовой узел) и одно нечистое подмножество. Снова разделим это нечистое подмножество на основе атрибута Age. Теперь у нас есть два чистых подмножества (листовой узел).

![](https://habrastorage.org/r/w1560/webt/lx/ah/a8/lxaha8qilcbqeasouc1fdp-bww8.png)

Итак, создано дерево решений с чистыми подмножествами.


### Преимущества и недостатки


Преимущества:

* Формируют четкие и понятные правила классификации. Например, «если возраст < 40 и нет имущества для залога, то отказать в кредите». То есть деревья решений хорошо и быстро интерпретируются.
* Способны генерировать правила в областях, где специалисту трудно формализовать свои знания.
* Легко визуализируются, то есть могут «интерпретироваться» не только как модель в целом, но и как прогноз для отдельного тестового субъекта (путь в дереве).
* Быстро обучаются и прогнозируют.
* Не требуется много параметров модели.
* Поддерживают как числовые, так и категориальные признаки.

Недостатки:

* Деревья решений чувствительны к шумам во входных данных. Небольшие изменения обучающей выборки могут привести к глобальным корректировкам модели, что скажется на смене правил классификации и интерпретируемости модели.
* Разделяющая граница имеет определенные ограничения, из-за чего дерево решений по качеству классификации уступает другим методам.
* Возможно переобучение дерева решений, из-за чего приходится прибегать к методу «отсечения ветвей», установке минимального числа элементов в листьях дерева или максимальной глубины дерева.
* Сложный поиск оптимального дерева решений: это приводит к необходимости использования эвристики типа жадного поиска признака с максимальным приростом информации, которые в конечном итоге не дают 100-процентной гарантии нахождения оптимального дерева.
* Дерево решений делает константный прогноз для объектов, находящихся в признаковом пространстве вне параллелепипеда, который охватывает не все объекты обучающей выборки.


**Преимущество дерева решений над например линейной регрессией:**  

![](https://i.imgur.com/rp8Xw1I.png)

В данном примере нельзя провести такую прямую, которая разделила бы выборку на объекты двух классов.

Согласно статье «Эмпирическое исследование настройки гиперпараметров деревьев решений» идеальные значения min_samples_split обычно находятся в диапазоне от 1 до 40 для алгоритма CART, который является алгоритмом, реализованным в scikit-learn. min_samples_split используется для управления переоснащением. Более высокие значения не позволяют модели изучать отношения, которые могут быть очень специфичными для конкретной выборки, выбранной для дерева. Слишком высокие значения также могут привести к недообучению, поэтому в зависимости от уровня недообучения или переобучения вы можете настроить значения для min_samples_split.